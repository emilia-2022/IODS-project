---
---
---

# Week 4 - Clustering and Classification

I have never worked with clustering or classification before this week. Therefore, it was incredibly challenging and I really struggled to understand how to interpret these results. I am not really sure why we used this set of this data set to understand crime when the data was related to housing values. Why not use it to look at housing values? I still am not really sure what the two clusters in the final output really represent, any good resources for interpretation would be appreciated!

```{r}
date()
```

```{r, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 4: Clustering and Classification

## Relevant Libraries and Packages

```{r, message = FALSE, warning = FALSE}
library(MASS)
library(tidyr)
library(corrplot)
library(tidyverse)
library(ggplot2)
library(lattice)    # for box plots 
library(reshape2)   # converting data to long format
library(factoextra) # k-means visualization 
library(ggpubr)     # k-means visualization 
library(GGally)     # ggpairs
```

## Step 1: Reading in the Data

```{r}
# load the data
data(Boston)

# explore the data set
str(Boston)
dim(Boston)
```

*Explanations*

Containing data on housing values in suburbs of Boston, this data set consists of 14 columns and 506 rows. Each variable can be considered one which influences housing value in these suburbs. Explanations for each of the variables can be seen below:

| Variable Name | Variable Type | Explanation                                                            |
|------------------|------------------|-------------------------------------|
| crim          | numeric       | per capita crime rate by town                                          |
| zn            | numeric       | proportion of residential land zoned for lots over 25,000 sq.ft        |
| indus         | numeric       | proportion of non-retail business acres per town                       |
| chas          | integer       | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). |
| nox           | numeric       | nitrogen oxides concentration (parts per 10 million)                   |
| rm            | numeric       | average number of rooms per dwelling.                                  |
| age           | numeric       | proportion of owner-occupied units built prior to 1940                 |
| dis           | numeric       | weighted mean of distances to five Boston employment centres           |
| rad           | integer       | index of accessibility to radial highways                              |
| tax           | numeric       | full-value property-tax rate per \$10,000                              |
| ptratio       | numeric       | pupil-teacher ratio by town                                            |
| black         | numeric       | 1000(Bkâˆ’0.63)2 where Bk is the proportion of blacks by town            |
| lstat         | numeric       | lower status of the population (percent)                               |
| medv          | numeric       | median value of owner-occupied homes in \$1000s                        |

## Step 2: Graphical and Numerical Exploration

### Graphical Overview

#### Boxplots

```{r, fig.width=9, warning=FALSE, message = FALSE}
# convert data to long format 
Boston_long <- melt(Boston)

# create boxplot for all variables 
bwplot(value ~ variable, Boston_long)
```

*Interpretations*

From this initial visualization it is clear that at least three variables (crim, zn and black) posses large numbers of outliers. However, due to the massive variance in scale across variables this is not an effective visualization. Therefore, the variables' distributions will be visualized in smaller box plot groups.

As we are interested in the variables distributions, we will create a data set containing those which are closest in scale and then visualize those.

```{r}
# remove columns with largest scale variance
box_boston = subset(Boston, select = -c(tax,black))
```

```{r}
boxplot(box_boston[1:3], col = "palegreen3", horizontal = TRUE)
```

*Interpretations*

A closer look at these boxplots confirms what was shown in the larger plot above. "Crim" and "zn" both possess many outliers and the distributions appear to be skewed to the right. Comparatively, "indus" seems to be distributed more evenly, although it appears to be left-leaning.

```{r}
boxplot(box_boston[4:6],  col = "palegreen3", horizontal = TRUE)
```

*Interpretations*

Above, we can see that "rm" possesses quite a few outliers with large tails on both sides of the distribution. Due to scale variance, it is hard to see the distribution of "nox" clearly. Despite this, it can appears to be left-leaning. "Chas" is a dummy variable, it's value is either 0-1 and is therefore not suited well to box plot visualization.

```{r}
boxplot(box_boston[7:9], col = "palegreen3", horizontal = TRUE)
```

*Interpretations*

"Rad" is a right-leaning distribution. It seems that the average house has has an index of access 10 to the radial highway. As for age, it appears as though around 70% of the buildings in these suburbs were built prior to 1940. The distribution has long tails which makes sense since it is in percent format. For "dis" some outliers appear on the right tail of the data.

```{r}
boxplot(box_boston[10:12], col = "palegreen3", horizontal = TRUE)
```

*Interpretations*

"Medv" seems to possess quite a few outliers, with an average around 20, meaning that the average median for owner occupied homes is around 20,000\$. "Lstat"also possess some outliers and appears to be left leaning. There seems to be an average of 14% of lower-status population in these suburbs (although I am not entirely sure what "lower-status population" refers to). Ptratio" seems to have a right-leaning distribution with a few outliers on the left tail.

#### Relationship

```{r, fig.height=6, fig.width=10}
pairs(Boston)
```

*Interpretations*

As this data uses such varying scales it is tough to comment on the relationship between variables in a meaningful way before scaling the data. However, from this scatterplot matrix it can be see that there appears to be a positive relationship between "rm" and "medv". This makes intuitive sense as the more rooms a house has, the higher value it should be. "Rm" also seems to have a negative relationship with "lstat", meaning that the higher the percentage lower-status population, the lower the average numer of rooms per home.

*Note:* For some reason I cannot get this visualization to knit in a way where it can be seen properly. If whoever is reading this knows how to format large visualizations like this for knitting, please let me know!

### Numerical Overview

#### Summaries

```{r}
summary(round(Boston, digits = 2))
```

*Interpretations*

Results from the numerical summary confirm the distributions which were visualized graphically above. Each variable has a varying scale which makes it hard to compare means across variables. However, we can see that both "tax" and "black" have large ranges which confirm their respective boxplots seen in the first visualization.

To make the data more comparable we will now standardize the data set.

### Step 3: Standardize data set & variable creation

#### Standardize

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

*Interpretations*

After standardizing, we can see that many variable values have become negative. The maximum values are also much closer across all variables. This step is crucial to perform Linear Discriminant Analysis (LDA) as LDA works under the assumption that each of the predictor variables have the same variance.

#### Categorical variable creation

```{r}
# create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, 
             breaks = bins, 
             include.lowest = TRUE, 
             labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

#### Train and Test set

```{r}
boston_scaled$crime <- factor(boston_scaled$crime, 
                              levels = c("low", 
                                         "med_low",
                                         "med_high", 
                                         "high"))

# choose randomly 80% of the rows in data set
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```

## Step 4: Linear Discriminant Analysis

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows
lda.arrows(lda.fit, myscale = 1)
```

*Interpretations*

From the length of the arrows we can see that "rad", "nox", and "zn" have the largest variation in the data set.Unfortunately, due to overlapping values it is hard to interpret the angles- which represent the relationship - between the arrows. Despite these difficulties, a potential right-angle can be seen between "rad" and "nox" meaning that they are not highly correlated.

## Step 5: Cross Tabulation

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

*Interpretations*

We can calculate the accuracy of the model following:

$$
Accuracy = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}
$$

$$
Accuracy = \frac{20 + 20 + 13 + 24}{102} = 0.75
$$

The model is accurate about 75% of the time. It appears that the model is very good at predicting levels of high crime with 0 predictions in the other categories. However, for low crime it often predicts higher than it should. The model performs similarly with med_low and med_high values with only 5 and 6 wrong predicted values respectively.

## Step 6: K-means clustering

### Reading in Data (again)

```{r}
# re-load the data
data(Boston)

# center and standardize variables
Boston_clustering <- scale(Boston)

Boston_clustering <- as.data.frame(Boston_clustering)
```

### Calculating Distances

```{r}
# euclidean distance matrix
dist_eu <- dist(Boston_clustering, method = "euclidean")

# manhattan distance matrix
dist_man <- dist(Boston_clustering, method = "manhattan")
```

### K-means Algorithm

#### Attempt One

```{r}
# k-means clustering
km <- kmeans(Boston_clustering, centers = 3)
```

#### Optimizing

```{r, warning = FALSE}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston_clustering, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# clean up visualization
fviz_nbclust(Boston_clustering, 
             kmeans, 
             nstart=100, 
             method = "wss") + 
  geom_vline(xintercept = 2, linetype = 1)
```

*Interpretations*

Looking at the "elbow" of the above plot, it seems like the optimal number of clusters is 2. Therefore, analysis will continue with 2 clusters.

### Visualizations

```{r}
km <- kmeans(Boston_clustering, centers = 2)

fviz_cluster(km, 
             data = Boston_clustering, 
             geom = c("point"),
             ellipse.type = "euclid", 
             ellipse.alpha = 0.1,
             ggtheme = theme_bw())

```

```{r, message=FALSE, warning=FALSE}
# create cluster as categorical variable for visualization 
Boston_clustering$km_cluster <- km$cluster
Boston_clustering <- Boston_clustering %>%
  mutate(
    cluster = km_cluster %>% factor() %>% 
      fct_recode("Cluster 1" = "1", 
                 "Cluster 2" = "2")
  )

# visualize (for readability ease only the tail of the data)
p <- ggpairs(Boston_clustering[c(8:14)],
             mapping = aes(col = Boston_clustering$cluster, alpha = 0.3), 
             lower = list(combo = wrap("facethist", bins = 20)))

p
```

*Interpretations*

I am not really sure how to interpret the first plot, but I think the center points were the centroids used for analysis. We can also see the spread of the clusters, but I am unsure what the x and y axis values represent. If anyone has any resources on how to interpret this output, please share them in the feedback!

From the second plot, we can see that for two variable pairs - "lstat" and "medv" and "rad" and "tax" both clusters are statistically significant. It can also be seen how the distributions differ between the two clusters quite clearly, For example, in the "tax" variable we can see that the distributions peak on almost opposite ends of the scale. I would guess that this means that those in cluster one (lower crime) have lower property tax rate and those in cluster two (higher crime) have higher property tax. However, I am not entirely sure of this interpretation!

## Bonus 1

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = Boston_clustering)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(km$cluster)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows
lda.arrows(lda.fit, myscale = 1)
```

*Interpretations*

Once again, not entirely sure of the interpretations. For the black cluster it appears as though"rad" are the most influential linear separators. Interpret the results. For the red cluster it could be that "zn" is an influential linear separator. However, with the overlapping of the other variables it is really hard to tell.
